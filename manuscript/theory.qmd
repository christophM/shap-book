# Theory of Shapley Values


## The Attribution Problem

Imagine a group of Meerkats and they are building a house.
In theory, each one of them could do it alone, but together they think they are faster.
So they build a house together and sell it.
Now it's time to pay out the worth of the house, which is determined by how large it is and how nice it looks.
But the 3 meerkats did different amount of hours.
So the first suggestion is to split it by hour.
But one of the Meerkats objects.
It's the Meerkat that put in the least amount of hours, but had great ideas that increased the value of the house greatly.
For starters it thought about making it a zero energy house, which now seems to be very trendy on the market.

The problem that the Meerkats are facing is one of attribution:
How do you fairly attribute the payout in a cooperative game?
Shapley values are an answer to such kind of question.
And as it turns out we can represent a prediction also as this type of game with an attribution problem at its core.

But let's start right away with an example from prediction. 

CONTINUE HERE

## For linear prediction problems

We are interested in how each feature affects the prediction of a data point.
In a linear model it is easy to calculate the individual effects.
Here is what a linear model prediction looks like for one data instance:

$$\hat{f}(x)=\beta_0+\beta_{1}x_{1}+\ldots+\beta_{p}x_{p}$$

where x is the instance for which we want to compute the contributions.
Each $x_j$ is a feature value, with j = 1,...,p.
The $\beta_j$ is the weight corresponding to feature j.

The contribution $\phi_j$ of the j-th feature on the prediction $\hat{f}(x)$ is:

$$\phi_j(\hat{f})=\beta_{j}x_j-E(\beta_{j}X_{j})=\beta_{j}x_j-\beta_{j}E(X_{j})$$

where $E(\beta_jX_{j})$ is the mean effect estimate for feature j.
The contribution is the difference between the feature effect minus the average effect.
Nice!
Now we know how much each feature contributed to the prediction.
If we sum all the feature contributions for one instance, the result is the following:

\begin{align*}\sum_{j=1}^{p}\phi_j(\hat{f})=&\sum_{j=1}^p(\beta_{j}x_j-E(\beta_{j}X_{j}))\\=&(\beta_0+\sum_{j=1}^p\beta_{j}x_j)-(\beta_0+\sum_{j=1}^{p}E(\beta_{j}X_{j}))\\=&\hat{f}(x)-E(\hat{f}(X))\end{align*}

This is the predicted value for the data point x minus the average predicted value.
Feature contributions can be negative.

Can we do the same for any type of model?
It would be great to have this as a model-agnostic tool.
Since we usually do not have similar weights in other model types, we need a different solution.

Help comes from unexpected places: cooperative game theory.
The Shapley value is a solution for computing feature contributions for single predictions for any machine learning model.


#### The Shapley Value

The Shapley value is defined via a value function $val$ of players in S.

The Shapley value of a feature value is its contribution to the payout, weighted and summed over all possible feature value combinations:

$$\phi_j(val)=\sum_{S\subseteq\{1,\ldots,p\} \backslash \{j\}}\frac{|S|!\left(p-|S|-1\right)!}{p!}\left(val\left(S\cup\{j\}\right)-val(S)\right)$$

where S is a subset of the features used in the model, x is the vector of feature values of the instance to be explained and p the number of features.
$val_x(S)$ is the prediction for feature values in set S that are marginalized over features that are not included in set S:

$$val_{x}(S)=\int\hat{f}(x_{1},\ldots,x_{p})d\mathbb{P}_{x\notin{}S}-E_X(\hat{f}(X))$$

You actually perform multiple integrations for each feature that is not contained S.
A concrete example:
The machine learning model works with 4 features x1, x2, x3 and x4 and we evaluate the prediction for the coalition S consisting of feature values x1 and x3:

$$val_{x}(S)=val_{x}(\{1,3\})=\int_{\mathbb{R}}\int_{\mathbb{R}}\hat{f}(x_{1},X_{2},x_{3},X_{4})d\mathbb{P}_{X_2X_4}-E_X(\hat{f}(X))$$

This looks similar to the feature contributions in the linear model!

Do not get confused by the many uses of the word "value":
The feature value is the numerical or categorical value of a feature and instance;
the Shapley value is the feature contribution to the prediction;
the value function is the payout function for coalitions of players (feature values).

The Shapley value is the only attribution method that satisfies the properties **Efficiency**, **Symmetry**, **Dummy** and **Additivity**,  which together can be considered a definition of a fair payout.

**Efficiency**
The feature contributions must add up to the difference of prediction for x and the average.

$$\sum\nolimits_{j=1}^p\phi_j=\hat{f}(x)-E_X(\hat{f}(X))$$

**Symmetry**
The contributions of two feature values j and k should be the same if they contribute equally to all possible coalitions.
If

$$val(S \cup \{j\})=val(S\cup\{k\})$$

for all

$$S\subseteq\{1,\ldots, p\} \backslash \{j,k\}$$

then

$$\phi_j=\phi_{k}$$

**Dummy**
A feature j that does not change the predicted value -- regardless of which coalition of feature values it is added to -- should have a Shapley value of 0.
If

$$val(S\cup\{j\})=val(S)$$

for all

$$S\subseteq\{1,\ldots,p\}$$

then

$$\phi_j=0$$

**Additivity**
For a game with combined payouts val+val^+^ the respective Shapley values are as follows:

$$\phi_j+\phi_j^{+}$$

Suppose you trained a random forest, which means that the prediction is an average of many decision trees.
The Additivity property guarantees that for a feature value, you can calculate the Shapley value for each tree individually, average them, and get the Shapley value for the feature value for the random forest.

#### Intuition

An intuitive way to understand the Shapley value is the following illustration:
The feature values enter a room in random order.
All feature values in the room participate in the game (= contribute to the prediction).
The Shapley value of a feature value is the average change in the prediction that the coalition already in the room receives when the feature value joins them.



### Advantages

The difference between the prediction and the average prediction is **fairly distributed** among the feature values of the instance -- the Efficiency property of Shapley values.
This property distinguishes the Shapley value from other methods such as [LIME](#lime).
LIME does not guarantee that the prediction is fairly distributed among the features.
The Shapley value might be the only method to deliver a full explanation.
In situations where the law requires explainability -- like EU's "right to explanations" -- the Shapley value might be the only legally compliant method, because it is based on a solid theory and distributes the effects fairly.
I am not a lawyer, so this reflects only my intuition about the requirements.

The Shapley value allows **contrastive explanations**.
Instead of comparing a prediction to the average prediction of the entire dataset, you could compare it to a subset or even to a single data point.
This contrastiveness is also something that local models like LIME do not have.

The Shapley value is the only explanation method with a **solid theory**.
The axioms -- efficiency, symmetry, dummy, additivity -- give the explanation a reasonable foundation.
Methods like LIME assume linear behavior of the machine learning model locally, but there is no theory as to why this should work.

It is mind-blowing to **explain a prediction as a game** played by the feature values.


### Disadvantages

The Shapley value requires **a lot of computing time**.
In 99.9% of real-world problems, only the approximate solution is feasible.
An exact computation of the Shapley value is computationally expensive because there are 2^k^ possible coalitions of the feature values and the "absence" of a feature has to be simulated by drawing random instances, which increases the variance for the estimate of the Shapley values estimation.
The exponential number of the coalitions is dealt with by sampling coalitions and limiting the number of iterations M.
Decreasing M reduces computation time, but increases the variance of the Shapley value.
There is no good rule of thumb for the number of iterations M.
M should be large enough to accurately estimate the Shapley values, but small enough to complete the computation in a reasonable time.
It should be possible to choose M based on Chernoff bounds, but I have not seen any paper on doing this for Shapley values for machine learning predictions.

The Shapley value **can be misinterpreted**.
The Shapley value of a feature value is not the difference of the predicted value after removing the feature from the model training.
The interpretation of the Shapley value is:
Given the current set of feature values, the contribution of a feature value to the difference between the actual prediction and the mean prediction is the estimated Shapley value.

The Shapley value is the wrong explanation method if you seek sparse explanations (explanations that contain few features).
Explanations created with the Shapley value method **always use all the features**.
Humans prefer selective explanations, such as those produced by LIME.
LIME might be the better choice for explanations lay-persons have to deal with.
Another solution is [SHAP](https://github.com/slundberg/shap) introduced by Lundberg and Lee (2016)[^lundberg2017], which is based on the Shapley value, but can also provide explanations with few features.

The Shapley value returns a simple value per feature, but **no prediction model** like LIME.
This means it cannot be used to make statements about changes in prediction for changes in the input, such as:
"If I were to earn €300 more a year, my credit score would increase by 5 points."

Another disadvantage is that **you need access to the data** if you want to calculate the Shapley value for a new data instance.
It is not sufficient to access the prediction function because you need the data to replace parts of the instance of interest with values from randomly drawn instances of the data.
This can only be avoided if you can create data instances that look like real data instances but are not actual instances from the training data.

Like many other permutation-based interpretation methods, the Shapley value method suffers from **inclusion of unrealistic data instances** when features are correlated.
To simulate that a feature value is missing from a coalition, we marginalize the feature.
This is achieved by sampling values from the feature's marginal distribution.
This is fine as long as the features are independent.
When features are dependent, then we might sample feature values that do not make sense for this instance.
But we would use those to compute the feature's Shapley value.
One solution might be to permute correlated features together and get one mutual Shapley value for them.
Another adaptation is conditional sampling: Features are sampled conditional on the features that are already in the team.
While conditional sampling fixes the issue of unrealistic data points, a new issue is introduced:
The resulting values are no longer the Shapley values to our game, since they violate the symmetry axiom, as found out by Sundararajan et al. (2019)[^cond1] and further discussed by Janzing et al. (2020)[^cond2].


### Software and Alternatives

Shapley values are implemented in both the `iml` and [fastshap](https://github.com/bgreenwell/fastshap) packages for R.
In Julia, you can use [Shapley.jl](https://gitlab.com/ExpandingMan/Shapley.jl).

SHAP, an alternative estimation method for Shapley values, is presented in the [next chapter](#shap).

Another approach is called breakDown, which is implemented in the `breakDown` R package[^breakdown].
BreakDown also shows the contributions of each feature to the prediction, but computes them step by step.
Let us reuse the game analogy:
We start with an empty team, add the feature value that would contribute the most to the prediction and iterate until all feature values are added.
How much each feature value contributes depends on the respective feature values that are already in the "team", which is the big drawback of the breakDown method.
It is faster than the Shapley value method, and for models without interactions, the results are the same.


[^shapley1953]: Shapley, Lloyd S. "A value for n-person games." Contributions to the Theory of Games 2.28 (1953): 307-317.

[^strumbelj2014]: Štrumbelj, Erik, and Igor Kononenko. "Explaining prediction models and individual predictions with feature contributions." Knowledge and information systems 41.3 (2014): 647-665.

[^breakdown]: Staniak, Mateusz, and Przemyslaw Biecek. "Explanations of model predictions with live and breakDown packages." arXiv preprint arXiv:1804.01955 (2018).

[^lundberg2017]: Lundberg, Scott M., and Su-In Lee. "A unified approach to interpreting model predictions." Advances in Neural Information Processing Systems (2017).

[^cond1]: Sundararajan, Mukund, and Amir Najmi. "The many Shapley values for model explanation." arXiv preprint arXiv:1908.08474 (2019).

[^cond2]: Janzing, Dominik, Lenon Minorics, and Patrick Blöbaum. "Feature relevance quantification in explainable AI: A causal problem." International Conference on Artificial Intelligence and Statistics. PMLR (2020).

### Advantages

Since SHAP computes Shapley values, all the advantages of Shapley values apply:
SHAP has a **solid theoretical foundation** in game theory.
The prediction is **fairly distributed** among the feature values.
We get **contrastive explanations** that compare the prediction with the average prediction.

SHAP **connects LIME and Shapley values**.
This is very useful to better understand both methods.
It also helps to unify the field of interpretable machine learning.

SHAP has a **fast implementation for tree-based models**.
I believe this was key to the popularity of SHAP, because the biggest barrier for adoption of Shapley values is the slow computation.

The fast computation makes it possible to compute the many Shapley values needed for the **global model interpretations**.
The global interpretation methods include feature importance, feature dependence, interactions, clustering and summary plots.
With SHAP, global interpretations are consistent with the local explanations, since the Shapley values are the "atomic unit" of the global interpretations.
If you use LIME for local explanations and partial dependence plots plus permutation feature importance for global explanations, you lack a common foundation.

### Disadvantages

**KernelSHAP is slow**.
This makes KernelSHAP impractical to use when you want to compute Shapley values for many instances.
Also all global SHAP methods such as SHAP feature importance require computing Shapley values for a lot of instances.

**KernelSHAP ignores feature dependence**.
Most other permutation based interpretation methods have this problem.
By replacing feature values with values from random instances, it is usually easier to randomly sample from the marginal distribution.
However, if features are dependent, e.g. correlated, this leads to putting too much weight on unlikely data points.
TreeSHAP solves this problem by explicitly modeling the conditional expected prediction.

**TreeSHAP can produce unintuitive feature attributions**.
While TreeSHAP solves the problem of extrapolating to unlikely data points, it does so by changing the value function and therefore slightly changes the game.
TreeSHAP changes the value function by relying on the conditional expected prediction.
With the change in the value function, features that have no influence on the prediction can get a TreeSHAP value different from zero.

The disadvantages of Shapley values also apply to SHAP:
Shapley values **can be misinterpreted** and access to data is needed to compute them for new data (except for TreeSHAP).

It is **possible to create intentionally misleading interpretations** with SHAP, which can hide biases [^fool].
If you are the data scientist creating the explanations, this is not an actual problem (it would even be an advantage if you are the evil data scientist who wants to create misleading explanations).
For the receivers of a SHAP explanation, it is a disadvantage: they cannot be sure about the truthfulness of the explanation.



