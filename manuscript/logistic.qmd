# Classification with Logistic Regression {#classification}

::: {.callout-tip appearance="simple"}

By the end of this chapter, you will be able to:

- Interpret SHAP for classification models.
- Decide whether to interpret probabilities or log odds.
- Handle categorical features.
- Use alternatives to the waterfall plot: the bar plot and the force plot.
- Describe the cluster plot and the heatmap plot.

:::

This chapter explains how to use and interpret SHAP with logistic regression.
Differences from linear regression include:

- Two outcomes instead of one.
- The outcome is either a probability [^no-probability] or log odds.
- A non-linear function when the output is a probability.

Logistic regression is suitable for binary classification tasks.
It provides probability outputs for two classes.
Since the probability of one class defines the other's, you can work with just one probability.
Having two classes is a special case of having $k$ classes.

::: {.callout-tip}

Even though the example here is binary classification, `shap` works the same for
multi-class models.

:::

From the SHAP perspective, classification is similar to regression, except the scale is the score output, not regression.

## The Adult dataset

We will use the Adult dataset from the UCI repository for the classification task.
This dataset contains demographic and socioeconomic data of individuals from the 1994 U.S. Census Bureau database, aiming to predict whether an individual's income is greater than or equal to \$50,000 per year.
The dataset includes features such as age, education level, work class, occupation, marital status, and more.
With approximately 32,000 observations, it contains both categorical and numerical features.
Conveniently, the `shap` package includes the Adult dataset, which simplifies its use in our example.

## Training the model

```{python}
import shap
from sklearn.model_selection import train_test_split

X, y = shap.datasets.adult()

X_train, X_test, y_train, y_test = train_test_split(
  X, y, test_size=0.2, random_state=1
)
```

Next, we train the model and compute the SHAP values.
Compared to the linear regression example, you will notice something new here:

```{python}
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.linear_model import LogisticRegression
import numpy as np

# Define the categorical and numerical features
cats = ['Workclass', 'Marital Status', 'Occupation',
        'Relationship', 'Race', 'Sex', 'Country']
nums = ['Age', 'Education-Num', 'Capital Gain',
        'Capital Loss', 'Hours per week']

# Define the column transformer
preprocessor = ColumnTransformer(
    transformers=[
        ('cat', OneHotEncoder(), cats),
        ('num', StandardScaler(), nums)
    ])

# Define the pipeline
model = Pipeline([
    ('preprocessor', preprocessor),
    ('classifier', LogisticRegression(max_iter=10000))
])

# Fit the pipeline to the training data
model.fit(X_train, y_train)

X_sub = shap.sample(X_train, 100)

ex = shap.Explainer(model.predict_proba, X_sub)
shap_values = ex(X_test.iloc[0:100])
```

The novelties include: 

- Using a subset of the training data as a background dataset, which can improve computation speed at the cost of less accurate SHAP value estimates.
- Applying SHAP not to the model but to the entire pipeline, allowing us to compute SHAP values for the original features instead of their processed versions.

The Adult dataset contains both categorical and numerical features, which are transformed before inputting them into the logistic regression model.

- Numerical features are standardized: $x_{j,std}^{(i)} = (x_j^{(i)} - \bar{x}_j) / sd(x_j)$.
- Categorical features are one-hot encoded. For instance, a feature with 1 column and 3 categories transforms into 3 columns, e.g. category "3" might be encoded as (0,0,1).


Following these steps, our dataset expands to approximately 470 columns.
The numerical features, like age, are no longer easily interpretable due to standardization, making it necessary to compute the actual age represented by, say, 0.8.

The logistic regression model, however, can process these inputs.
This implies that the coefficients are based on this transformed dataset.
Applying SHAP values directly on the logistic regression model would yield 470 SHAP values.

Yet, there's a more interpretable method:
We can integrate the preprocessing and logistic regression into a pipeline and regard it as our model.
This approach is akin to nesting mathematical functions:

- Our model is $y = f(\tilde{x})$, where $\tilde{x}$ is the preprocessed data.
- We have our preprocessing steps, denoted as $g$, so the preprocessed data can be expressed as $\tilde{x} = g(x)$.
- A less desirable option would be to apply SHAP values on $f$.
- A better choice is to define a new function $\tilde{f}(x) = f(g(x))$, where the input is $x$ and not $\tilde{x}$. $\tilde{f}$ represents the pipeline.
- We apply and interpret SHAP values for $\tilde{f}$ instead of $f$.

This method facilitates the interpretation of features in their original form.

::: {.callout-tip}

When preprocessing your data, think about which steps you want to incorporate into your pipeline when calculating SHAP values.
It's sensible to include steps like feature standardization in the pipeline, while transformations that enhance interpretability should be left out.

:::

Another point of interest is that the model's output is 2-dimensional, not 1-dimensional.
This is mirrored in the resulting `shap_values` variable, which gains an extra dimension.
Hence, we also need to specify the class for which we want to obtain the SHAP values.

```{python}
class_index = 0
data_index = 1

shap.plots.waterfall(shap_values[data_index,:,class_index])
```

For this individual, the predicted likelihood of earning more than \$50k was 99.9%, well above the expected 79%.
This plot shows that Marital Status (4 = married) was the most influential feature, contributing 0.06.
The interpretation is largely the same as for regression, except that the outcome is on the probability level and we have to choose which class's SHAP values we want to interpret.

Now, let's inspect the SHAP values for the alternative class.

```{python}
class_index = 1
shap.plots.waterfall(shap_values[data_index,:,class_index])
```

This plot mirrors the previous one, except all SHAP values are multiplied by -1.
This is logical since the probabilities for both classes must add up to 1.
Thus, a variable that increases the classification by 0.11 for class >50k decreases the classification by 0.11 for class <=50k.
We only need to pick one of the two classes.
This changes when we have three or more classes, see the [Image Chapter](#image) for an example involving multiple classes.


## Alternatives to the waterfall plot

The waterfall plot visualizes the SHAP values of a data instance.
However, there are other ways to visualize the exact same type of information: the bar plot and the force plot.
If I were to arrange the 3 plots on a spectrum, I'd say that the bar plot is the most conventional, familiar to most people, followed by the waterfall plot and the force plot, which is the most challenging to read.

Let's begin with the bar plot.

```{python}
shap.plots.bar(shap_values[data_index,:,class_index])
```
The interpretation here is the same as that of the waterfall plot, so I will not repeat it. 
The only difference between the two plots is the arrangement of information, with the bar plot lacking in the presentation of $\mathbb{E}(f(X))$ and $f(x^{(i)}$.

Additionally, the force plot is simply a different representation of the SHAP values:

```{python}
#| eval: False
shap.initjs()
shap.plots.force(shap_values[data_index,:,class_index])
```

![Force Plot](images/force-plot.png)

The force plot, interactive and based on JavaScript, allows you to hover over it for more insights.
Of course, this feature is not available in the static format you're currently viewing, but it can be accessed if you create your own plot and embed it in a Jupyter notebook or a website.
The image above is a screenshot of a force plot.
The plot is named force plot because the SHAP values are depicted as forces, represented by arrows, which can either increase or decrease the prediction.
If you compare it with the waterfall plot, it's like a horizontal arrangement of arrows.

Personally, I find the waterfall plot easier to read than the force plot, and it provides more information than the bar plot.

## Interpreting log odds

In logistic regression, it's common to interpret the model in terms of log odds rather than probability.

The explainer has a `link` argument for this, which defaults to the identity link $l(x) = x$.
For classification, the logit link: $l(x) = log\left(\frac{x}{1 - x}\right)$, is a good choice.
We can use this logit link to transform the output of the logistic regression model and compute SHAP values on this new scale: 

```{python}
ex_logit = shap.Explainer(
  model.predict_proba, X_sub, link=shap.links.logit
)
sv_logit = ex_logit(X_test.iloc[0:100])
class_index = 0

shap.plots.waterfall(sv_logit[data_index,:,class_index])
```

When the outcome of a logistic regression model is defined in terms of log odds, the features impact the outcome linearly.
In simpler terms, the logistic regression model is a linear model on the level of the log odds. 

Here's what it means for interpretation:
A marital status of 0 (married) contributes +1.47 to the log odds of making >50k versus <=50k compared to the average prediction.
However, SHAP values shine in their applicability at the probability level, and log odds can be challenging to interpret.
So, when should you use log odds and when should you use probabilities?

::: {.callout-note}

If your focus is on the probability outcome, use the identity link (which is the default behavior).
The logit space is more suitable if you're interested in "evidence" in an information-theoretic sense, even if the effect in probability space isn't substantial.

:::

Let's discuss when the distinction between log odds and probabilities matters:
A shift from 80% to 90% is significant in probability space, while a change from 98% to 99.9% is relatively minor.

In probability space, the differences are 0.10 and 0.019.
In logit space, we have:

- $log(0.9/0.1) - log(0.8/0.2) \approx 0.8$ and
- $log(0.999/0.001) - log(0.98/0.02) \approx 3$

In logit space, the step is significantly larger.
This happens because the logit compresses near 0 and 1, making changes in the middle of probability space appear larger.

So, which one should you select?
If you're primarily concerned with probabilities, and a jump from 50% to 51% is as significant to you as from 99% to 100%, stick with the default and use the identity link.
However, if changes in extreme probabilities near 0 and 1 are more critical for your application, choose logits.
You can also visualize the difference in step sizes in the following @fig-logits.

![Probabilities versus Logits](images/logits.jpg){#fig-logits}

## Understanding the data globally

To finish up the model interpretation, let's have a look at the global importances and effects with the summary plot.
Here we interpret the model in the probability space again.

```{python}
shap.plots.beeswarm(shap_values[:,:,class_index])
```

From our observations, Marital Status and Education emerge as the two most significant features on a log-odds scale.
For some individuals, Capital Gain has substantial effects, suggesting that high values of capital gain result in small SHAP values.

## Clustering SHAP values

Clustering can be applied to your data using SHAP values.
The objective of clustering is to identify groups of similar instances.
Typically, clustering relies on features, which are often of different scales.
For instance, height may be measured in meters, color intensity from 0 to 100, and some sensor output between -1 and 1.
The tricky part is calculating distances between instances with such diverse, non-comparable features.

SHAP clustering operates by clustering the SHAP values of each instance, meaning that instances are clustered by their explanation similarity.
All SHAP values share the same unit â€” the unit of the prediction space.
Any clustering method can be employed.
The following example utilizes hierarchical agglomerative clustering to sort the instances.

The plot comprises numerous force plots, each explaining the prediction of an instance.
We rotate the force plots vertically and arrange them side by side according to their clustering similarity.
It's a Javascript plot.
Here's how it looks for the first 21 data points:

```{python}
#| eval: False
shap.plots.force(sv_logit[0:20:,:,0])
```


![Clustering Plot](images/clustering-plot-1.png)

::: {.callout-note}

## Cluster plot

- The cluster plot consists of vertical force plots.
- Data instances are distributed across the x-axis, while SHAP values are spread across the y-axis.
- Color signifies the direction of SHAP values.
- The larger the area for a feature, the larger the SHAP values across the data, indicating the significance of this feature.
- By default, the data instances are arranged by their similarity in SHAP values.

:::

You can hover over the plot for more information, change what you see on the x-axis, and experiment with various other orderings.
The cluster plot is an exploratory tool.

You can also alter the ordering, for example, by the prediction:

![Clustering Plot](images/clustering-plot-2.png)


## The heatmap plot

The heatmap plot is another tool for global interpretation.
Unlike the dependence or importance plot, the heatmap plot displays all SHAP values without aggregation.

```{python}
shap.plots.heatmap(sv_logit[:,:,class_index])
```

::: {.callout-note}

## Heatmap plot

- Each row on the y-axis represents a feature, and instances are distributed across the x-axis.
- Color signifies the SHAP value.
- Instances are arranged based on clustered SHAP values.
- The curve at the top displays the predicted value for the data.

:::

Some features are automatically summarized, a plotting behavior that can be managed by setting the `max_display` argument.

[^no-probability]: While the output is a number between 0 and 1, classifiers are frequently not well calibrated, so be cautious when interpreting the output as a probability in real-world scenarios.
